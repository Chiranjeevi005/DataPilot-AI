This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```                                                                                    

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.

---

## Frontend Features

### ğŸ¨ Responsive Design Overhaul
- **Mobile-First Experience**: Fully optimized for phones, tablets, and desktops using fluid layouts and typography.
- **Adaptive Navigation**:
  - **Mobile**: Hamburger menu with slide-over sidebar and simplified header.
  - **Tablet**: Mini-icon sidebar using tooltips for maximum screen real estate.
  - **Desktop**: Persistent sidebar with improved data density.
- **Smart Components**:
  - **Preview Table**: Automatically switches from scrolling table (Desktop) to card view (Mobile) with expandable details.
  - **Charts**: Responsive Recharts that adjust axis density, font size, and margins based on viewport.
  - **File Upload**: Touch-friendly large buttons for mobile, drag-and-drop zone for desktop.
- **Accessibility**: ARIA labels, focus management, and keyboard navigation support.

### ğŸ“Š Interactive Dashboard
- **Real-Time Progress**: Animated step-by-step loading state with job cancellation.
- **Dynamic Results**: Interactive charts and KPI cards adapting to data context.
- **Insight Panel**: AI-generated business insights displayed alongside data.

---

## Backend (Python)

The backend implementation for DataPilot AI Phase 1 handles file uploads and job queuing.

### Prerequisites

1.  **Python 3.11+** installed.
2.  **Redis** installed and running (or use Docker).

### Setup

1.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **Configure Environment:**
    Copy `env.example` to `.env` and adjust if necessary.
    ```bash
    cp env.example .env
    ```
    *Note: `.env` is typically gitignored.*

3.  **Start Redis (Docker):**
    ```bash
    docker run --name redis-server -p 6379:6379 -d redis
    ```

### Running the Server

Start the standalone Python dev server for the upload endpoint:
```bash
# Make sure project root is in PYTHONPATH if needed, or run as module
# Windows Powershell:
$env:PYTHONPATH="."
python src/api/upload/route.py

# Linux/Mac:
export PYTHONPATH=.
python src/api/upload/route.py
```
The server runs on `http://localhost:5328` by default.

### Testing

Run the provided test script (requires bash/curl):
```bash
./scripts/test_upload.sh
```

Or manually:
```bash
# Upload CSV
# Note: On Windows PowerShell, use 'curl.exe' instead of 'curl'
curl -F "file=@./dev-samples/sales_demo.csv" http://localhost:5328/api/upload

```

### File Storage & Inspection

- **Local Development**: Files are saved to `tmp_uploads/` in the project root (Windows friendly) or `/tmp/uploads` (Linux/Mac) if `BLOB_ENABLED=false`.
- **Blob Storage**: Toggle `BLOB_ENABLED=true` in `.env` to enable (requires valid `BLOB_KEY` and SDK implementation).

### API Endpoints

- **POST /api/upload**
    - **Multipart**: `file=@path/to/file.csv`
    - **JSON**: `{"fileUrl": "...", "filename": "..."}`
    - **Returns**: `{"jobId": "job_...", "status": "submitted"}`

---

## Backend Worker (Phase 3)

The specific worker process handles `data_jobs` from Redis, processing them (simulation) and generating results.

### Running the Worker

The worker listens to Redis queue `data_jobs`.

```bash
# Using helper script
./scripts/run_worker.sh

# Or manually
export PYTHONPATH=$PYTHONPATH:$(pwd)/src
python src/worker.py
```

### Testing the Worker

1. Start Redis.
2. Start the Upload API (see above) if performing full flow.
3. Start the Worker (in a separate terminal).
4. Run the test flow:
   ```bash
   ./scripts/test_worker_flow.sh
   ```

### Worker Configuration

The worker uses the following `.env` settings:
- `SIMULATED_WORK_SECONDS`: Duration of simulated processing (default: 3).
- `WORKER_POLL_INTERVAL`: Redis polling interval in seconds (default: 1).
- `RESULTS_DIR`: Directory to store result JSONs if not using Blob (default: `/tmp/datapilot/results` or relative to project on Windows).

---

## Backend Phase 4: Data Processing & EDA

The Phase 4 worker implements real data processing for CSV, XLSX, and JSON files.

### Features
- **File Parsing**: Robust parsing with encoding fallback (UTF-8/Latin-1) and error handling.
- **Schema Inference**: Detects types (date, numeric, categorical, etc.), missing counts, and unique counts.
- **KPIs**: Row/Col counts, duplicates, and numeric stats (sum, mean, max, etc.).
- **Smart Charts**: Automatically generates specs for time-series (Line) and categorical distributions (Bar).
- **Quality Score**: Heuristic score (0-100) based on missingness, duplicates, and consistency.

### Result JSON Structure
The worker attempts to generate a `result.json` saved to storage:

```json
{
  "jobId": "job_...",
  "fileInfo": { "name": "data.csv", "rows": 100, "cols": 5, "type": "csv" },
  "schema": [
    { "name": "Date", "inferred_type": "datetime", "missing_count": 0, "unique_count": 9, "sample_values": ["2023-01-01"] }
  ],
  "kpis": {
    "rowCount": 100,
    "colCount": 5,
    "missingCount": 2,
    "numDuplicates": 0,
    "numericStats": { "Revenue": { "sum": 50000.0, "mean": 500.0, ... } }
  },
  "cleanedPreview": [
    { "Date": "2023-01-01", "Revenue": 500 }
  ],
  "chartSpecs": [
    { "id": "chart_timeseries_1", "type": "line", "xKey": "date", "yKey": "value", "data": [...] }
  ],
  "qualityScore": 85,
  "processedAt": "2025-12-06T..."
}
```

### Quality Score Logic
- **Base**: 100
- **Penalties**:
  - Missing Values: - (Percent Missing * 30), capped at 30 points.
  - Duplicates: -10 points if > 1% rows are duplicates.
- **Result**: `max(0, 100 - penalties)`

### Testing Phase 4
Run the automated test which uploads a messy CSV and validates the output:
```bash
./scripts/test_phase4.sh
```

---

## Backend Phase 6: LLM Insights (Gemini)

The Phase 6 worker integration adds automated AI business insights using Gemini.

### Setup
1. **API Key**: Set `GEMINI_API_KEY` in `.env`.
   - Get key from [Google AI Studio](https://aistudio.google.com/).
2. **Model**: Optional `GEMINI_MODEL` (default: `gemini-1.5-flash`).

### Features
- **Business Summary**: 3-5 high-level insights derived from EDA (KPIs, Schema, Preview).
- **Evidence**: Structured citations (aggregates or specific row examples) backing the insights.
- **Fail-Safe**:
  - **Retry Logic**: Automatically retries on JSON parsing errors.
  - **Fallback**: Returns a graceful "Analysis unavailable" message if API fails, ensuring the job still completes.
  - **Mock Mode**: Set `LLM_MOCK=true` to simulate LLM responses for local dev/testing without costs.

### Reprocessing
To re-run the LLM analysis on an *already processed* job (e.g. to iterate on prompts):
```bash
./scripts/reprocess_job.sh <jobId>
```
The script reads the existing `result.json`, re-sends context to LLM, and updates the insights in-place.

### Replacing Gemini
To use a different provider (e.g. OpenAI, Anthropic):
1. Modify `src/lib/llm_client.py`.
2. Update the `generate_insights` function to call the new provider.
3. Ensure the prompt logic remains consistent (Inputs: Schema/KPIs, Output: JSON).

---

## Backend Phase 7: Production Hardening

Phase 7 makes the DataPilot backend production-ready with retry/backoff policies, job timeouts, cancellation, and structured error handling.

### Features

#### 1. Retry & Backoff Logic
All transient operations use exponential backoff with jitter:
- **Blob Operations**: 3 attempts (configurable via `BLOB_RETRY_ATTEMPTS`)
- **LLM Calls**: 2 attempts (configurable via `LLM_RETRY_ATTEMPTS`)
- **HTTP Downloads**: 3 attempts

**Algorithm**:
```
delay = min(max_delay, initial_delay * factor^(attempt-1))
delay *= random.uniform(0.9, 1.1)  # Â±10% jitter
```

**Configuration** (`.env`):
```bash
BLOB_RETRY_ATTEMPTS=3
LLM_RETRY_ATTEMPTS=2
RETRY_INITIAL_DELAY=0.5
RETRY_FACTOR=2.0
RETRY_MAX_DELAY=10
```

#### 2. LLM Circuit Breaker
Prevents cascading failures when LLM service is down:
- Tracks consecutive failures within a sliding time window
- Opens circuit after threshold failures
- Uses deterministic fallback during cooldown period
- Automatically closes after cooldown

**Configuration**:
```bash
LLM_CIRCUIT_THRESHOLD=5        # failures to open circuit
LLM_CIRCUIT_WINDOW=300         # seconds (5 min)
LLM_CIRCUIT_COOLDOWN=600       # seconds (10 min)
```

**Behavior**:
- Circuit OPEN â†’ All LLM calls use fallback (no API calls)
- Circuit CLOSED â†’ Normal operation with retries
- Fallback provides generic insights based on KPIs

#### 3. Job Timeout
Server-enforced timeout prevents runaway jobs:
- Set at job creation (`timeoutAt` field)
- Checked periodically during processing
- Enforced by `/api/job-status` endpoint

**Configuration**:
```bash
JOB_TIMEOUT_SECONDS=600  # 10 minutes default
```

**Behavior**:
- Job exceeds timeout â†’ status set to `failed`
- Error code: `timeout`
- `error.json` created in blob storage

#### 4. Job Cancellation
Cancel jobs via API endpoint:

**Endpoint**: `POST /api/cancel?jobId=<id>`

**Request**:
```bash
curl -X POST "http://localhost:5328/api/cancel?jobId=job_abc123"
# or JSON body
curl -X POST http://localhost:5328/api/cancel \
  -H "Content-Type: application/json" \
  -d '{"jobId": "job_abc123"}'
```

**Response**:
```json
{
  "jobId": "job_abc123",
  "status": "cancelled",
  "cancelledAt": "2025-12-06T10:30:00Z"
}
```

**Behavior**:
- Worker checks cancellation before and during processing
- Cancelled jobs do NOT produce `result.json`
- Cannot cancel completed/failed jobs

#### 5. Error Handling & error.json
On failure, an `error.json` is saved to blob storage:

**Structure**:
```json
{
  "jobId": "job_abc123",
  "status": "failed",
  "error": "timeout",
  "message": "Job exceeded maximum processing time",
  "timestamp": "2025-12-06T10:30:00Z"
}
```

**Error Codes**:
- `timeout` - Job exceeded `JOB_TIMEOUT_SECONDS`
- `processing_error` - General processing failure
- `blob_upload_failed` - Storage operation failed after retries
- `worker_shutdown` - Worker terminated during processing

**Job Status Fields**:
```json
{
  "jobId": "job_abc123",
  "status": "failed",
  "error": "timeout",
  "errorMessage": "Job exceeded maximum processing time",
  "resultUrl": "file:///path/to/error.json",
  "createdAt": "2025-12-06T10:00:00Z",
  "updatedAt": "2025-12-06T10:10:00Z"
}
```

#### 6. Client Polling Strategy
Recommended exponential backoff for status polling:

**Algorithm**:
```javascript
const delays = [1000, 2000, 4000, 8000, 15000]; // ms
let attempt = 0;
const maxWait = 600000; // 10 minutes

while (elapsed < maxWait) {
  const status = await fetchJobStatus(jobId);
  
  if (status === 'completed' || status === 'failed' || status === 'cancelled') {
    break;
  }
  
  const delay = delays[Math.min(attempt, delays.length - 1)];
  await sleep(delay);
  attempt++;
}
```

**Configuration**:
```bash
CLIENT_MAX_WAIT_SECONDS=600  # 10 minutes
```

### Testing Phase 7

Four test scripts validate production hardening:

#### 1. Blob Retry Test
```bash
./scripts/test_phase7_retry_blob.sh
```
Tests retry logic for blob operations (requires manual failure injection for full test).

#### 2. LLM Failure Test
```bash
./scripts/test_phase7_llm_fail.sh
```
Tests LLM retry and fallback by using invalid API key.

**Expected**:
- 2 retry attempts
- Deterministic fallback used
- Job completes with `status: completed`
- `issues` field contains LLM failure indicator

#### 3. Cancellation Test
```bash
./scripts/test_phase7_cancel.sh
```
Tests job cancellation at various stages.

**Expected**:
- Cancelled jobs remain `status: cancelled`
- No `result.json` produced
- Cannot cancel completed jobs

#### 4. Timeout Test
```bash
./scripts/test_phase7_timeout.sh
```
Tests server-side timeout enforcement.

**Expected**:
- Jobs exceeding timeout marked as `failed`
- Error code: `timeout`
- `error.json` created

### Graceful Shutdown
Worker handles SIGINT/SIGTERM gracefully:
- Completes current processing step
- Marks in-progress job as `failed` with `error: worker_shutdown`
- Exits cleanly

**Test**:
```bash
# Start worker
python src/worker.py

# In another terminal, send SIGTERM
kill -TERM <worker_pid>
```

### Monitoring & Observability
All operations log structured messages:
```json
{
  "timestamp": "2025-12-06T10:30:00Z",
  "level": "INFO",
  "jobId": "job_abc123",
  "step": "llm_call",
  "message": "LLM call (deepseek/deepseek-r1): Attempt 1/2",
  "duration_ms": 1234
}
```

**Key Log Messages**:
- `"Attempt {n}/{max}"` - Retry attempts
- `"Circuit breaker OPENING"` - Circuit breaker triggered
- `"Job {id} was cancelled"` - Cancellation detected
- `"Job {id} has timed out"` - Timeout enforced

### Production Deployment Checklist

- [ ] Set `JOB_TIMEOUT_SECONDS` appropriate for workload
- [ ] Configure `OPENROUTER_API_KEY` for LLM
- [ ] Set circuit breaker thresholds based on expected load
- [ ] Enable blob storage (`BLOB_ENABLED=true`)
- [ ] Set up Redis persistence
- [ ] Configure worker auto-restart (systemd, supervisor, etc.)
- [ ] Set up log aggregation (CloudWatch, Datadog, etc.)
- [ ] Monitor circuit breaker state
- [ ] Set up alerts for high failure rates
- [ ] Test graceful shutdown in production environment

### Environment Variables Reference

See `.env.example` for complete list. Key Phase 7 variables:

```bash
# Timeouts
JOB_TIMEOUT_SECONDS=600
CLIENT_MAX_WAIT_SECONDS=600

# Retry Configuration
LLM_RETRY_ATTEMPTS=2
BLOB_RETRY_ATTEMPTS=3
RETRY_INITIAL_DELAY=0.5
RETRY_FACTOR=2.0
RETRY_MAX_DELAY=10

# Circuit Breaker
LLM_CIRCUIT_THRESHOLD=5
LLM_CIRCUIT_WINDOW=300
LLM_CIRCUIT_COOLDOWN=600

# LLM
OPENROUTER_API_KEY=your_key_here
LLM_MODEL=deepseek/deepseek-r1
```

---

## Backend Phase 8: Few-Shot Prompt Pipeline & Fine-Tuning

Phase 8 introduces a production-ready prompt + dataset pipeline that:
- **Immediately** improves LLM outputs using few-shot learning with DeepSeek R1
- **Collects** human-approved examples for fine-tuning datasets
- **Exports** clean JSONL datasets ready for instruction-tuning

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Few-Shot Pipeline                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Compact EDA â†’ Prompt Manager                            â”‚
â”‚     â†“                                                        â”‚
â”‚  2. Select 3-8 relevant few-shot examples (similarity)      â”‚
â”‚     â†“                                                        â”‚
â”‚  3. Build prompt: System + Few-Shots + Current EDA          â”‚
â”‚     â†“                                                        â”‚
â”‚  4. Call LLM (DeepSeek R1, temp=0.0)                        â”‚
â”‚     â†“                                                        â”‚
â”‚  5. Validate output (schema, evidence, normalization)       â”‚
â”‚     â†“                                                        â”‚
â”‚  6. If invalid â†’ Retry with "fix structure" instruction     â”‚
â”‚     â†“                                                        â”‚
â”‚  7. If still invalid â†’ Fallback to deterministic templates  â”‚
â”‚     â†“                                                        â”‚
â”‚  8. Return validated insights                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Human-in-the-Loop Feedback                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Reviewer approves/edits/rejects insights (CLI or API)   â”‚
â”‚     â†“                                                        â”‚
â”‚  2. Feedback saved to data/feedback/                        â”‚
â”‚     â†“                                                        â”‚
â”‚  3. Collect approved examples (score â‰¥ 3)                   â”‚
â”‚     â†“                                                        â”‚
â”‚  4. Export to JSONL (instruction-tuning format)             â”‚
â”‚     â†“                                                        â”‚
â”‚  5. Validate dataset (schema, PII, duplicates)              â”‚
â”‚     â†“                                                        â”‚
â”‚  6. Upload to fine-tuning platform                          â”‚
â”‚     â†“                                                        â”‚
â”‚  7. Fine-tune model â†’ Deploy â†’ Evaluate vs few-shot         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

#### 1. Similarity-Based Few-Shot Selection
Dynamically selects most relevant examples based on input characteristics:
- **Date column presence**: Prioritizes time-series examples
- **Missing data %**: Matches similar data quality scenarios
- **Outlier presence**: Detects max/min ratio > 10
- **Strong correlations**: Identifies |r| > 0.7
- **Duplicate presence**: Matches data quality issues

**Result**: More relevant context â†’ Better LLM outputs

#### 2. PII Masking
Automatically masks sensitive information before sending to LLM:
- **Emails**: `user@example.com` â†’ `u***@example.com`
- **Phones**: `555-123-4567` â†’ `555-***-4567`
- **SSNs**: `123-45-6789` â†’ `***-**-****`
- **Credit Cards**: `4111 1111 1111 1111` â†’ `4111 **** **** 1111`

**Result**: Safe to use with external LLM APIs

#### 3. Validation-First Approach
Every LLM response is validated before acceptance:
- **Schema validation**: Ensures correct JSON structure
- **Evidence sanity checks**: Validates aggregates, row_indices, chart_ids
- **Normalization**: Converts to canonical values (severity, who, priority)
- **Structural repair**: Attempts to fix common LLM mistakes
- **Fallback on failure**: Uses deterministic templates if validation fails

**Result**: 95%+ schema validation pass rate

#### 4. Audit Logging
All LLM calls are logged for monitoring:
- **Prompt hash**: SHA-256 for deduplication (not content)
- **Model name**: e.g., `deepseek/deepseek-r1`
- **Duration**: Call latency in seconds
- **Validation outcome**: Pass/fail with issue count
- **Stored in**: `data/llm_logs/llm_audit_YYYYMMDD.jsonl`

**Result**: Full observability without logging PII

### Directory Structure

```
datapilot-ai/
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ system_prompt.txt          # Comprehensive system instruction
â”‚   â”œâ”€â”€ fewshot_examples.json      # 8 curated few-shot examples
â”‚   â””â”€â”€ analyst_prompt.txt         # (Legacy, replaced by system_prompt)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ prompt_manager.py      # Few-shot selection & prompt building
â”‚   â”‚   â”œâ”€â”€ insight_validator.py   # Schema validation & normalization
â”‚   â”‚   â”œâ”€â”€ llm_client_fewshot.py  # LLM client with few-shot support
â”‚   â”‚   â””â”€â”€ fallback_generator.py  # Deterministic template insights
â”‚   â””â”€â”€ collectors/
â”‚       â””â”€â”€ feedback_collector.py  # Human feedback collection
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collect_finetune_examples.py    # Export JSONL for fine-tuning
â”‚   â”œâ”€â”€ validate_finetune_dataset.py    # Validate JSONL quality
â”‚   â””â”€â”€ eval_fewshot_vs_finetuned.py    # Compare performance
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ feedback/                  # Human feedback files
â”‚   â”œâ”€â”€ finetune_ready/            # Exported JSONL datasets
â”‚   â”œâ”€â”€ finetune_samples/          # Seed examples (10 provided)
â”‚   â””â”€â”€ llm_logs/                  # LLM audit logs
â”œâ”€â”€ reports/                       # Validation & evaluation reports
â””â”€â”€ docs/
    â””â”€â”€ finetune_playbook.md       # Complete fine-tuning guide
```

### Setup

#### 1. Environment Variables

Add to `.env`:

```bash
# Few-Shot Configuration
FEWSHOT_DEFAULT_COUNT=3           # Number of few-shot examples (1-8)

# LLM Configuration (already exists from Phase 7)
OPENROUTER_API_KEY=your_key_here
LLM_MODEL=deepseek/deepseek-r1
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_MOCK=false                    # Set true for testing without API calls

# Retry & Circuit Breaker (already exists from Phase 7)
LLM_RETRY_ATTEMPTS=2
LLM_CIRCUIT_THRESHOLD=5
LLM_CIRCUIT_WINDOW=300
LLM_CIRCUIT_COOLDOWN=600
```

#### 2. Install Dependencies

```bash
pip install -r requirements.txt
# Includes: openai, redis, pandas, pdfplumber
```

### Usage

#### Running with Few-Shot Pipeline

The few-shot pipeline is **automatically enabled** when you run the worker:

```bash
# Start worker (uses few-shot prompts by default)
python src/worker.py
```

**What happens**:
1. Worker receives job from Redis queue
2. Runs EDA analysis (Phase 4-5)
3. Calls `llm_client_fewshot.generate_insights_fewshot()`
4. Prompt manager selects 3 relevant few-shot examples
5. Builds prompt: system + few-shots + compact EDA
6. Calls DeepSeek R1 (temp=0.0)
7. Validates output with `insight_validator`
8. If validation fails â†’ Retry with fix instruction
9. If still fails â†’ Fallback to deterministic templates
10. Saves validated insights to `result.json`

#### Collecting Human Feedback

Use the CLI tool to review insights:

```bash
cd src/collectors
python feedback_collector.py <job_id> <path_to_result.json>
```

**Example**:
```bash
python feedback_collector.py job_abc123 ../../tmp_uploads/results/result_abc123.json
```

**Interactive workflow**:
- Review each insight
- Choose: `[a]` Approve, `[e]` Edit, `[r]` Reject, `[s]` Skip
- Provide quality score (1-5)
- Feedback saved to `data/feedback/`

#### Exporting Fine-Tuning Dataset

Collect approved examples into JSONL:

```bash
cd scripts

# Export all approved examples (score â‰¥ 3)
python collect_finetune_examples.py

# Export only high-quality (score â‰¥ 4)
python collect_finetune_examples.py --min-score 4

# Sample 50 examples for quick experiment
python collect_finetune_examples.py --sample 50
```

**Output**: `data/finetune_ready/finetune_YYYYMMDD.jsonl`

#### Validating Dataset

Before uploading to fine-tuning platform:

```bash
cd scripts
python validate_finetune_dataset.py ../data/finetune_ready/finetune_20241206.jsonl
```

**Checks**:
- âœ… Schema correctness
- âœ… No PII leakage
- âœ… Size and token estimates
- âœ… Duplicate detection
- âœ… Quality score (0-100)

**Output**: `reports/finetune_export_report_TIMESTAMP.json`

#### Evaluating Performance

Compare few-shot vs fine-tuned model:

```bash
cd scripts
python eval_fewshot_vs_finetuned.py \
  --holdout ../data/finetune_samples/holdout_test.jsonl \
  --finetuned-endpoint https://api.openai.com/v1/chat/completions
```

**Metrics**:
- Schema validation pass rate
- Evidence mapping accuracy
- BLEU/ROUGE scores (optional)

### Testing

#### Test Few-Shot Pipeline (Mock Mode)

```bash
# Set mock mode in .env
LLM_MOCK=true

# Run worker
python src/worker.py

# Upload test file
curl -F "file=@./dev-samples/sales_demo.csv" http://localhost:5328/api/upload

# Check result.json (should have mock insights)
```

#### Test with Real LLM

```bash
# Set real API key in .env
OPENROUTER_API_KEY=your_key_here
LLM_MOCK=false

# Run worker
python src/worker.py

# Upload file and check insights quality
```

#### Test Validation & Fallback

```bash
# Use invalid API key to trigger fallback
OPENROUTER_API_KEY=invalid_key
LLM_MOCK=false

# Run worker
python src/worker.py

# Upload file
# Expected: Fallback insights generated, job completes successfully
```

### Fine-Tuning Workflow

**Complete guide**: See `docs/finetune_playbook.md`

**Quick start**:

1. **Collect 50-100 jobs** with human feedback
2. **Export dataset**: `python scripts/collect_finetune_examples.py`
3. **Validate**: `python scripts/validate_finetune_dataset.py <jsonl>`
4. **Upload to platform** (OpenAI, Anthropic, Hugging Face)
5. **Fine-tune** with recommended hyperparameters:
   - Learning rate: `5e-6` to `1e-5`
   - Epochs: `3-5`
   - Batch size: `4-8`
   - Validation split: `10-15%`
6. **Evaluate**: `python scripts/eval_fewshot_vs_finetuned.py`
7. **Deploy** fine-tuned model and monitor performance

**Expected improvements**:
- Schema pass rate: 95% â†’ 98%+
- Evidence accuracy: 85% â†’ 95%+
- Reduced few-shot examples needed: 3 â†’ 0-1

### Prompt Engineering

#### System Prompt

Located in `prompts/system_prompt.txt`, defines:
- **Role**: DataPilot AI Senior Analyst
- **Output schema**: Strict JSON with analystInsights, businessSummary, visualActions, metadata
- **Evidence rules**: aggregates, row_indices, chart_id validation
- **Quality standards**: Specificity, context, actionability
- **Common scenarios**: Outliers, missing data, correlations, trends, etc.

**Customization**: Edit `prompts/system_prompt.txt` to adjust behavior

#### Few-Shot Examples

Located in `prompts/fewshot_examples.json`, contains 8 curated examples:
1. **Outlier detection** (revenue spike)
2. **Missing data** (email field)
3. **Correlation** (marketing spend vs revenue)
4. **Duplicates** (order records)
5. **Seasonality** (weekend sales pattern)
6. **Category imbalance** (regional distribution)
7. **Data quality** (ambiguous date formats)
8. **Trend reversal** (revenue decline)

**Customization**: Add domain-specific examples to improve relevance

#### Similarity Scoring

Prompt manager uses these features for similarity:
- `has_date_column` (weight: 1.5)
- `has_outliers` (weight: 2.0)
- `has_strong_correlation` (weight: 1.5)
- `has_duplicates` (weight: 1.0)
- `missing_pct_bucket` (weight: 1.5)
- `row_count_bucket` (weight: 0.5)

**Result**: Deterministic selection of most relevant examples

### Security & Privacy

#### PII Masking

All prompts are automatically masked before sending to LLM:
- Emails, phones, SSNs, credit cards â†’ Masked patterns
- Implemented in `prompt_manager._mask_pii()`
- Validation script checks for unmasked PII in datasets

#### Audit Logging

LLM calls are logged **without** storing raw prompt content:
- **Logged**: Prompt hash, model, duration, validation outcome
- **Not logged**: Raw prompt text (may contain PII)
- **Location**: `data/llm_logs/llm_audit_YYYYMMDD.jsonl`

#### Secrets Management

- **API keys**: Stored in `.env` (gitignored)
- **Feedback**: Stored locally in `data/feedback/` (gitignored)
- **Fine-tune datasets**: Validate for PII before uploading

### Monitoring & Observability

#### Key Metrics

Track these in production:
- **Schema validation pass rate**: Target >95%
- **Evidence accuracy rate**: Target >90%
- **Fallback usage rate**: Target <5%
- **LLM latency**: P50, P95, P99
- **Circuit breaker state**: Open/closed
- **Human approval rate**: Target >80%

#### Log Messages

```
INFO - Built few-shot prompt: 4500 chars, hash: a1b2c3d4
INFO - Selected 3 few-shot examples with similarities: [0.85, 0.72, 0.68]
INFO - Calling LLM (deepseek/deepseek-r1)...
INFO - LLM call succeeded in 2.34s with 0 validation issues
WARNING - Validation failed critically: [missing 'analystInsights']
INFO - Retrying with structural fix instruction...
ERROR - LLM failed after retries: Invalid JSON
INFO - Using deterministic fallback
```

#### Audit Log Format

```json
{
  "timestamp": 1701864000.0,
  "job_id": "job_abc123",
  "prompt_hash": "a1b2c3d4e5f6g7h8",
  "model": "deepseek/deepseek-r1",
  "duration_seconds": 2.34,
  "validation_passed": true,
  "issue_count": 0,
  "issues": []
}
```

### Troubleshooting

#### Low Schema Validation Rate (<90%)

**Symptoms**: Many insights fail validation

**Causes**:
- System prompt unclear
- Few-shot examples inconsistent
- LLM temperature too high

**Fix**:
1. Review `prompts/system_prompt.txt` for clarity
2. Ensure few-shot examples follow exact schema
3. Verify `temperature=0.0` in LLM calls

#### High Fallback Usage (>10%)

**Symptoms**: Many jobs use deterministic fallback

**Causes**:
- LLM API issues
- Circuit breaker open
- Invalid API key

**Fix**:
1. Check `OPENROUTER_API_KEY` validity
2. Review circuit breaker state in logs
3. Check LLM API status (OpenRouter dashboard)

#### Poor Fine-Tuned Model Performance

**Symptoms**: Fine-tuned model worse than few-shot

**Causes**:
- Insufficient training data (<50 examples)
- Poor quality feedback (low scores)
- Hyperparameters too aggressive

**Fix**:
1. Collect 200+ high-quality examples
2. Review feedback guidelines with reviewers
3. Lower learning rate, reduce epochs
4. See `docs/finetune_playbook.md` for details

### Best Practices

#### Prompt Engineering
- âœ… Keep system prompt concise but comprehensive
- âœ… Use exact schema in few-shot examples
- âœ… Cover diverse scenarios (outliers, quality, correlations, etc.)
- âœ… Update few-shot examples based on common failure modes

#### Feedback Collection
- âœ… Review 10-20 jobs per week
- âœ… Involve multiple reviewers for diversity
- âœ… Approve only high-quality insights (score â‰¥ 4)
- âœ… Edit instead of reject when possible (preserves examples)

#### Fine-Tuning
- âœ… Start with 50-100 examples
- âœ… Validate dataset quality (score >80)
- âœ… Use holdout set for evaluation (10-15%)
- âœ… Re-train quarterly or when 100+ new examples collected

#### Production Deployment
- âœ… Monitor schema validation rate daily
- âœ… Set up alerts for circuit breaker opening
- âœ… Track human approval rates
- âœ… Review audit logs weekly for anomalies

### Performance Benchmarks

**Few-Shot Pipeline** (DeepSeek R1, 3 examples):
- Schema validation: **95-98%**
- Evidence accuracy: **85-92%**
- Avg latency: **2-4 seconds**
- Fallback rate: **2-5%**

**Fine-Tuned Model** (after 200+ examples):
- Schema validation: **98-99%**
- Evidence accuracy: **92-97%**
- Avg latency: **1.5-3 seconds**
- Fallback rate: **<1%**

### Environment Variables Reference

```bash
# Few-Shot Configuration
FEWSHOT_DEFAULT_COUNT=3           # Number of few-shot examples (1-8)

# LLM Configuration
OPENROUTER_API_KEY=your_key_here
LLM_MODEL=deepseek/deepseek-r1
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_MOCK=false

# Retry Configuration (from Phase 7)
LLM_RETRY_ATTEMPTS=2
RETRY_INITIAL_DELAY=0.5
RETRY_FACTOR=2.0
RETRY_MAX_DELAY=10

# Circuit Breaker (from Phase 7)
LLM_CIRCUIT_THRESHOLD=5
LLM_CIRCUIT_WINDOW=300
LLM_CIRCUIT_COOLDOWN=600
```

### Next Steps

1. **Week 1**: Run worker with few-shot pipeline, collect feedback on 50 jobs
2. **Week 2**: Export dataset, validate quality (target score >80)
3. **Week 3**: Upload to fine-tuning platform, train first model
4. **Week 4**: Evaluate performance, iterate based on results

**Long-term**: Aim for 500+ examples over 3 months, re-train quarterly, maintain >95% validation rate.

**Documentation**: See `docs/finetune_playbook.md` for complete fine-tuning guide.

---

## Cleanup & Retention Subsystem

DataPilot AI includes an automated cleanup system that removes temporary uploads, results, and Redis job keys older than a configurable TTL (Time-To-Live). This ensures efficient storage usage, protects user privacy, and maintains system performance.

### Features

- **Automated Cleanup**: Scheduled job runs daily (configurable) to remove old data
- **TTL-Based Retention**: Configurable retention period (default: 24 hours)
- **Dry-Run Mode**: Safe testing without actual deletion
- **Audit Trail**: Every run creates a detailed audit log
- **Batch Limits**: Prevents runaway deletion with configurable batch size
- **Safety Guardrails**: Multiple validation checks prevent accidental data loss
- **Health Monitoring**: Health check endpoint for system status
- **Idempotent**: Safe to run multiple times

### What Gets Cleaned

1. **Upload Files**: `uploads/{jobId}/*` - User-uploaded files
2. **Result Files**: `results/{jobId}.json` and `results/{jobId}_error.json`
3. **Redis Job Keys**: `job:{jobId}` - Job metadata and status

### Quick Start

#### 1. Configuration

Add to `.env`:

```bash
# Retention Configuration
JOB_TTL_HOURS=24                              # How long to keep data
CLEANER_DRY_RUN=true                          # Dry-run mode (safety)
CLEANER_CRON_SCHEDULE="0 3 * * *"            # Daily at 3 AM UTC
CLEANER_MAX_DELETE_BATCH=500                  # Max items per run

# Safety Settings
MIN_TTL_HOURS=1                               # Minimum allowed TTL
BLOB_PATH_PREFIXES="uploads/,results/"        # Paths to clean
CLEANER_AUDIT_BLOB_PATH="maintenance/cleaner_runs/"
CLEANER_LOG_LEVEL=INFO
```

#### 2. Manual Run (Dry-Run)

Test the cleaner without deleting anything:

```bash
# Dry-run mode (safe)
python -m src.maintenance.cron_entry
```

Check the audit log:
```bash
cat tmp_uploads/maintenance/cleaner_runs/cleaner_*.json | tail -1 | python -m json.tool
```

#### 3. Enable Destructive Deletion

âš ï¸ **Warning**: This will permanently delete data!

```bash
# Edit .env
CLEANER_DRY_RUN=false

# Run cleaner
python -m src.maintenance.cron_entry
```

**Best Practice**: Run in dry-run mode for at least 7 days before enabling destructive deletion.

### Scheduled Job Setup

#### For Antigravity Platform

Add to `antigravity.yaml`:

```yaml
scheduled_jobs:
  - name: datapilot-cleanup
    schedule: "0 3 * * *"  # Daily at 3 AM UTC
    command: python -m src.maintenance.cron_entry
    runtime: python3.11
    timeout: 600
    environment:
      REDIS_URL: ${REDIS_URL}
      JOB_TTL_HOURS: "24"
      CLEANER_DRY_RUN: "true"
```

#### For Standard Cron (Linux/Mac)

```bash
# Add to crontab
0 3 * * * cd /path/to/datapilot-ai && python -m src.maintenance.cron_entry >> /var/log/datapilot-cleanup.log 2>&1
```

#### For Windows Task Scheduler

1. Open Task Scheduler
2. Create Basic Task: "DataPilot AI Cleanup"
3. Trigger: Daily at 3:00 AM
4. Action: `python -m src.maintenance.cron_entry`
5. Start in: `C:\path\to\datapilot-ai`

### Testing

Run the complete test suite:

```bash
# Create fake test data
python scripts/test_cleaner_create_fake_data.py

# Run full test suite (dry-run + destructive)
pwsh scripts/test_cleaner_run.sh

# Run edge case tests (validation, idempotency, etc.)
pwsh scripts/test_cleaner_edgecases.sh
```

**Test Coverage**:
- âœ… Dry-run doesn't delete data
- âœ… Destructive mode deletes old data only
- âœ… Recent data is preserved
- âœ… Redis keys cleaned correctly
- âœ… Audit logs created
- âœ… TTL validation enforced
- âœ… Prefix validation enforced
- âœ… Batch limits respected
- âœ… Error handling continues processing

### Health Check

Monitor cleanup subsystem health:

```bash
python -m src.maintenance.health_check
```

Returns JSON status:
```json
{
  "status": "healthy",
  "timestamp": "2025-12-06T12:00:00Z",
  "checks": {
    "redis": {"ok": true, "message": "Redis connection successful"},
    "blobPermissions": {"ok": true, "message": "Blob listing successful"}
  },
  "lastCleanerRun": {
    "completedAt": "2025-12-06T03:00:15Z",
    "deletedBlobs": 45,
    "deletedKeys": 30,
    "errors": 0,
    "dryRun": true
  }
}
```

### Audit Logs

Every cleanup run creates an audit log:

**Location**: `tmp_uploads/maintenance/cleaner_runs/cleaner_{timestamp}.json`

**Format**:
```json
{
  "runId": "cleaner_20251206_030000",
  "startedAt": "2025-12-06T03:00:00Z",
  "completedAt": "2025-12-06T03:00:15Z",
  "dryRun": true,
  "ttlHours": 24,
  "totalBlobsScanned": 150,
  "deletedBlobs": 45,
  "totalKeysScanned": 100,
  "deletedKeys": 30,
  "skipped": [],
  "errors": []
}
```

### Safety Features

1. **Dry-Run Default**: Defaults to `CLEANER_DRY_RUN=true` for safety
2. **Minimum TTL**: Prevents TTL < `MIN_TTL_HOURS` (default: 1 hour)
3. **Path Validation**: Requires `uploads/` and `results/` in `BLOB_PATH_PREFIXES`
4. **Batch Limits**: Max `CLEANER_MAX_DELETE_BATCH` items per run
5. **Error Isolation**: If one deletion fails, others continue
6. **Idempotency**: Safe to run multiple times

### Troubleshooting

#### No Data Being Deleted

1. Check `CLEANER_DRY_RUN=false` if you want actual deletion
2. Verify data is older than `JOB_TTL_HOURS`
3. Review audit logs

#### Permission Errors

1. Check storage directory permissions
2. Verify Redis connectivity
3. Run health check: `python -m src.maintenance.health_check`

#### Too Much Data Being Deleted

1. Increase `JOB_TTL_HOURS`
2. Check system clock is correct (UTC)
3. Review audit logs for unexpected deletions

### Documentation

- **Complete Guide**: `docs/retention_policy.md`
- **Scheduled Jobs**: `docs/scheduled_jobs.md`

### Environment Variables Reference

```bash
# Retention Configuration
JOB_TTL_HOURS=24                              # Retention period
MIN_TTL_HOURS=1                               # Minimum TTL (safety)
BLOB_PATH_PREFIXES="uploads/,results/"        # Paths to clean

# Cleanup Schedule
CLEANER_CRON_SCHEDULE="0 3 * * *"            # Cron schedule
CLEANER_DRY_RUN=true                          # Dry-run mode
CLEANER_LOG_LEVEL=INFO                        # Log verbosity
CLEANER_MAX_DELETE_BATCH=500                  # Max deletions per run

# Audit Trail
CLEANER_AUDIT_BLOB_PATH="maintenance/cleaner_runs/"
```

### Best Practices

1. **Start with dry-run**: Test with `CLEANER_DRY_RUN=true` for at least 7 days
2. **Monitor audit logs**: Review logs regularly for the first week
3. **Set appropriate TTL**: Balance storage costs with user needs
4. **Schedule off-peak**: Run cleanup during low-traffic hours
5. **Alert on errors**: Set up monitoring for cleanup failures

---

## Observability & Monitoring

DataPilot AI includes a comprehensive observability layer for production monitoring, debugging, and operational insights.

### Features

#### 1. Structured JSON Logging
All logs follow a consistent JSON format with:
- UTC ISO-8601 timestamps
- Component identification
- Job/request tracking
- Processing step markers
- PII masking (emails, phones, IDs)

**Example Log**:
```json
{
  "timestamp": "2025-12-06T12:00:00Z",
  "level": "INFO",
  "component": "process_job",
  "jobId": "job_abc123",
  "step": "eda",
  "message": "EDA completed",
  "extra": {"qualityScore": 85, "rows": 1000}
}
```

#### 2. Metrics Collection
Tracks key performance indicators:
- **Counters**: jobs_received_total, jobs_completed_total, jobs_failed_total, llm_failures_total, blob_failures_total
- **Histograms**: avg_processing_time_seconds (with p50, p95, p99 percentiles)
- Auto-flush to blob storage or local file
- Configurable flush interval

#### 3. Health Check Endpoint
`GET /api/health` provides runtime diagnostics:
- **Redis**: Connectivity check
- **Blob Storage**: Configuration validation
- **Worker**: Heartbeat monitoring (< 60s age)

**Response**:
```json
{
  "status": "ok",
  "components": {
    "redis": {"status": "ok"},
    "blob": {"status": "ok"},
    "worker": {"status": "ok", "details": {"lastHeartbeat": "2025-12-06T12:00:00Z"}}
  }
}
```

#### 4. Worker Heartbeat
Background thread updates Redis every 30 seconds:
- Key: `worker:heartbeat`
- Value: UTC timestamp
- TTL: 120 seconds
- Health check validates age < 60 seconds

#### 5. Sentry Integration (Optional)
Automatic error tracking with:
- Exception capture with stack traces
- Context: jobId, step, requestId
- PII masking before sending
- Environment tagging

### Setup

#### 1. Basic Configuration

Add to `.env`:
```bash
# Logging
OBSERVABILITY_LOG_LEVEL=INFO              # DEBUG, INFO, WARNING, ERROR
DEBUG=false                                # Enable debug mode (dev only!)

# Metrics
METRICS_FLUSH_INTERVAL=10                 # Flush every N jobs
METRICS_AUTO_FLUSH=true                   # Auto-flush in background

# Worker Heartbeat
WORKER_HEARTBEAT_INTERVAL=30              # Seconds
HEALTH_WORKER_MAX_AGE_SECONDS=60          # Max heartbeat age

# Optional: Sentry
SENTRY_DSN=                                # Leave empty to disable
ENVIRONMENT=development                    # For Sentry tagging
```

#### 2. Enable Sentry (Optional)

```bash
# Install SDK
pip install sentry-sdk

# Configure DSN
SENTRY_DSN=https://your-dsn@sentry.io/project-id

# Restart worker
python src/worker.py
```

### Usage

#### View Logs

```bash
# Run worker with JSON output
python src/worker.py | jq

# Filter by job
python src/worker.py | jq 'select(.jobId == "job_123")'

# Filter by step
python src/worker.py | jq 'select(.step == "llm")'
```

#### Check Health

```bash
# Start health server
python src/api/health/route.py

# Check status
curl http://localhost:5329/api/health | jq
```

#### View Metrics

```bash
# Local file
cat metrics/metrics_snapshot.json | jq

# Check counters
cat metrics/metrics_snapshot.json | jq '.counters'

# Check processing time stats
cat metrics/metrics_snapshot.json | jq '.histograms.avg_processing_time_seconds'
```

### Testing

```bash
# Test API logging
python scripts/test_api_logging.py

# Test worker metrics
python scripts/test_worker_metrics.py

# Test health endpoint
python scripts/test_health_endpoint.py
```

### Log Coverage

Each job produces **13+ structured log entries**:
1. Job dequeued
2. Job started
3. Status update
4. Blob read
5. File type detection
6. Parsing
7. EDA start
8. EDA completed
9. LLM start
10. LLM completed
11. Result writing
12. Result saved
13. Job completed (with duration)

### PII Masking

Automatically masks sensitive data:
- **Emails**: `john@example.com` â†’ `[EMAIL_MASKED]`
- **Phones**: `555-123-4567` â†’ `[PHONE_MASKED]`
- **IDs**: `ABCD12345678` â†’ `[ID_MASKED]`

### Monitoring Best Practices

1. **Health Checks**: Monitor `/api/health` every 60 seconds
2. **Metrics Review**: Check metrics snapshots daily
3. **Alert on Failures**: Set up alerts for:
   - Worker heartbeat stale (> 60s)
   - Redis connection errors
   - High failure rates (> 10%)
4. **Log Aggregation**: Send logs to centralized system (ELK, CloudWatch)
5. **Sentry Monitoring**: Review errors daily in production

### Documentation

- **Full Guide**: `docs/observability.md`
- **Quick Reference**: `OBSERVABILITY_QUICK_REFERENCE.md`
- **Implementation Summary**: `OBSERVABILITY_IMPLEMENTATION_COMPLETE.md`

### Environment Variables Reference

```bash
# Observability
SENTRY_DSN=                                # Optional: Sentry DSN
OBSERVABILITY_LOG_LEVEL=INFO               # Log level
METRICS_FLUSH_INTERVAL=10                  # Jobs between flushes
METRICS_AUTO_FLUSH=true                    # Auto-flush enabled
METRICS_SNAPSHOT_PATH=metrics/metrics_snapshot.json
DEBUG=false                                 # Debug mode (never in production!)
WORKER_HEARTBEAT_INTERVAL=30               # Heartbeat interval (seconds)
HEALTH_WORKER_MAX_AGE_SECONDS=60           # Max heartbeat age (seconds)
ENVIRONMENT=development                    # Environment name
```

### Troubleshooting

#### No Logs Appearing
- Check `OBSERVABILITY_LOG_LEVEL` is set
- Verify observability module is imported
- Check stdout/stderr output

#### Metrics Not Saving
- Verify `METRICS_AUTO_FLUSH=true`
- Check `METRICS_SNAPSHOT_PATH` is writable
- Review worker logs for flush errors

#### Worker Showing as Stale
- Verify worker is running
- Check Redis connectivity
- Review `WORKER_HEARTBEAT_INTERVAL` setting

#### Sentry Not Working
- Verify `SENTRY_DSN` is correct
- Check `sentry-sdk` is installed
- Look for initialization message in logs

---

## Deployment to Antigravity

DataPilot AI is production-ready and can be deployed to Antigravity with comprehensive automation, smoke tests, and monitoring.

### Quick Start

```bash
# 1. Set required secrets
export OPENROUTER_API_KEY="sk-or-v1-your-key-here"
export REDIS_URL="redis://your-redis:6379/0"

# 2. Deploy
cd scripts
./deploy_antigravity.sh

# 3. Verify
./verify_deploy.sh

# 4. Run smoke tests
./run_smoke_tests.sh
```

### Deployment Features

- âœ… **One-Command Deployment**: `./deploy_antigravity.sh`
- âœ… **Automated Verification**: Health checks, Redis, blob storage, worker heartbeat
- âœ… **Comprehensive Smoke Tests**: 6 end-to-end tests with exponential backoff
- âœ… **Automatic Rollback**: On smoke test failure (CI/CD)
- âœ… **Secrets Management**: Antigravity Secret Manager integration
- âœ… **CI/CD Ready**: GitHub Actions workflow included
- âœ… **Monitoring**: Structured logging, metrics, alerts
- âœ… **Documentation**: Runbook, troubleshooting, checklist

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Antigravity Platform                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  API Service â”‚  â”‚    Worker    â”‚  â”‚   Cleaner    â”‚     â”‚
â”‚  â”‚  (Serverless)â”‚  â”‚  (Persistent)â”‚  â”‚  (Scheduled) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                  â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    Redis Queue                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                  Blob Storage                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   OpenRouter     â”‚
                    â”‚ (deepseek-r1)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment Scripts

| Script | Purpose |
|--------|---------|
| `deploy_antigravity.sh` | Main deployment with validation and secrets |
| `rollback_antigravity.sh` | Rollback to previous deployment |
| `verify_deploy.sh` | Post-deployment health checks |
| `run_smoke_tests.sh` | Comprehensive smoke test suite |
| `ci_deploy.sh` | CI/CD integration with auto-rollback |

### Smoke Tests

Six comprehensive end-to-end tests:

1. **Upload and Process**: Upload CSV â†’ Process â†’ Validate result.json schema
2. **Frontend Render**: Validate result payload structure
3. **LLM Integration**: Verify OpenRouter LLM calls and insights
4. **Cancel Flow**: Upload â†’ Cancel â†’ Verify status transitions
5. **Error Handling**: Test LLM failure fallback
6. **Timeout**: Test job timeout enforcement

**Features**:
- Exponential backoff polling (1s â†’ 2s â†’ 4s â†’ 8s â†’ 15s)
- Configurable timeout (`CLIENT_MAX_WAIT_SECONDS`)
- Detailed reports saved to `reports/`

### Rollback

```bash
# Automatic rollback (with confirmation)
./scripts/rollback_antigravity.sh

# Force rollback (no confirmation)
FORCE_ROLLBACK=1 ./scripts/rollback_antigravity.sh

# Rollback to specific version
ROLLBACK_VERSION=deploy_20241206_100000 ./scripts/rollback_antigravity.sh
```

### CI/CD Integration

GitHub Actions workflow (`.github/workflows/deploy.yml`):

- âœ… Automated deployment on push to `main`/`staging`
- âœ… Python syntax validation
- âœ… Smoke tests after deployment
- âœ… Automatic rollback on failure
- âœ… Slack notifications
- âœ… Artifact uploads

**Required GitHub Secrets**:
- `OPENROUTER_API_KEY`
- `REDIS_URL`
- `BLOB_KEY`
- `SENTRY_DSN`
- `ANTIGRAVITY_API_ENDPOINT`
- `ANTIGRAVITY_API_KEY`
- `SLACK_WEBHOOK_URL`

### Monitoring

**Metrics**:
- `jobs_received_total` - Total jobs received
- `jobs_completed_total` - Total jobs completed
- `jobs_failed_total` - Total jobs failed
- `job_processing_duration_seconds` - Job processing time
- `llm_call_duration_seconds` - LLM call latency

**Alerts**:
- High failure rate (> 10%)
- Worker down (heartbeat age > 120s)
- LLM circuit breaker open

**Logs**:
```bash
# View worker logs
antigravity logs --service worker --follow

# View API logs
antigravity logs --service api --follow
```

### Documentation

- **[Deployment Guide](./docs/DEPLOYMENT.md)** - Quick start and architecture
- **[Deployment Runbook](./docs/deploy_runbook.md)** - Detailed procedures
- **[Troubleshooting Guide](./docs/troubleshooting.md)** - Common issues
- **[Deployment Checklist](./docs/deployment_checklist.md)** - Pre/post-deployment
- **[Deployment Complete](./DEPLOYMENT_COMPLETE.md)** - Implementation summary

### Environment Variables

Required secrets (set in Antigravity Secret Manager):

```bash
OPENROUTER_API_KEY=sk-or-v1-your-key  # Required
REDIS_URL=redis://your-redis:6379/0   # Required
BLOB_KEY=your-blob-key                # Optional
SENTRY_DSN=https://your-sentry-dsn    # Optional
```

Configuration (set in `antigravity.yml`):

```bash
LLM_MODEL=deepseek/deepseek-r1
JOB_TIMEOUT_SECONDS=600
MAX_UPLOAD_SIZE_BYTES=20971520
WORKER_HEARTBEAT_INTERVAL=30
CLEANER_CRON_SCHEDULE="0 3 * * *"
```

### Acceptance Criteria

Deployment is successful when:

- âœ… `deploy_antigravity.sh` runs successfully
- âœ… `verify_deploy.sh` returns healthy
- âœ… `run_smoke_tests.sh` passes all tests
- âœ… Upload returns jobId for sample CSV
- âœ… Worker processes job and sets status to `completed`
- âœ… `result.json` exists and matches schema
- âœ… Logs show LLM calls to OpenRouter
- âœ… No active alerts
- âœ… Error rate < 5%

### Next Steps

1. Set secrets in Antigravity Secret Manager
2. Run `./scripts/deploy_antigravity.sh`
3. Verify with `./scripts/verify_deploy.sh`
4. Test with `./scripts/run_smoke_tests.sh`
5. Monitor metrics and logs for 24 hours
6. Configure CI/CD with GitHub Actions

---

